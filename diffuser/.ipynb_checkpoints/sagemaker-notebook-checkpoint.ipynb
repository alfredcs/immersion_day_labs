{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d9ec49a",
   "metadata": {},
   "source": [
    "# Stable Diffusion on Amazon SageMaker\n",
    "\n",
    "Welcome to this Amazon SageMaker guide on how to use the [Stable Diffusion](https://huggingface.co/blog/stable_diffusion) to generate image for a given input prompt. We will deploy [CompVis/stable-diffusion-v1-4](https://huggingface.co/CompVis/stable-diffusion-v1-4) to Amazon SageMake for real-time inference using Hugging Faces [ðŸ§¨ Diffusers library](https://huggingface.co/docs/diffusers/index).\n",
    "\n",
    "![stable-diffusion-on-amazon-sagemaker](https://raw.githubusercontent.com/huggingface/notebooks/main/sagemaker/23_stable_diffusion_inference/imgs/sd-on-sm.png)\n",
    "\n",
    "What we are going to do \n",
    "1. Create Stable Diffusion inference script \n",
    "2. Create SageMaker `model.tar.gz` artifact\n",
    "3. Deploy the model to Amazon SageMaker\n",
    "4. Generate images using the deployed model\n",
    "\n",
    "\n",
    "## What is Stable Diffusion?\n",
    "\n",
    "Stable Diffusion is a text-to-image latent diffusion model created by the researchers and engineers from [CompVis](https://github.com/CompVis), [Stability AI](https://stability.ai/) and [LAION](https://laion.ai/). It is trained on 512x512 images from a subset of the [LAION-5B](https://laion.ai/blog/laion-5b/) database. LAION-5B is the largest, freely accessible multi-modal dataset that currently exists.\n",
    "\n",
    "This guide will not explain how the model works. If you are interested you should checkout the [Stable Diffusion with ðŸ§¨ Diffusers\n",
    "](https://huggingface.co/blog/stable_diffusion) blog post or [The Annotated Diffusion Model](https://huggingface.co/blog/annotated-diffusion)\n",
    "\n",
    "\n",
    "![stable-diffusion](https://raw.githubusercontent.com/huggingface/notebooks/main/sagemaker/23_stable_diffusion_inference/imgs/stable-diffusion-arch.png)\n",
    "\n",
    "--- \n",
    "\n",
    "Before we can get started, make sure you have [Hugging Face user account](https://huggingface.co/join). The account is needed to load the [CompVis/stable-diffusion-v1-4](https://huggingface.co/CompVis/stable-diffusion-v1-4) from the [Hugging Face Hub](https://huggingface.co/).\n",
    "\n",
    "Create account: https://huggingface.co/join\n",
    "\n",
    "Before we can get started we have to install the missing dependencies to be able to create our `model.tar.gz` artifact and create our Amazon SageMaker endpoint. \n",
    "We also have to make sure we have the permission to create our SageMaker Endpoint.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69c59d90",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install \"sagemaker>=2.116.0\" \"huggingface_hub>=0.10.1\" --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4386d9",
   "metadata": {},
   "source": [
    "_If you are going to use Sagemaker in a local environment (not SageMaker Studio or Notebook Instances). You need access to an IAM Role with the required permissions for Sagemaker. You can find [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) more about it._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c22e8d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::415275363822:role/service-role/AmazonSageMakerServiceCatalogProductsUseRole\n",
      "sagemaker bucket: sagemaker-us-east-1-415275363822\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce4bb37",
   "metadata": {},
   "source": [
    "## Create Stable Diffusion inference script \n",
    "\n",
    "Amazon SageMaker allows us to customize the inference script by providing a `inference.py` file. The `inference.py` file is the entry point to our model. It is responsible for loading the model and handling the inference request. If you are used to deploying Hugging Face Transformers that might be knew to you. Usually, we just provide the `HF_MODEL_ID` and `HF_TASK` and the Hugging Face DLC takes care of the rest. For `diffusers` thats not yet possible. We have to provide the `inference.py` file and implement the `model_fn` and `predict_fn` functions. \n",
    "\n",
    "If you want to learn more about creating a custom inference script you can check out [Creating document embeddings with Hugging Face's Transformers & Amazon SageMaker](https://www.philschmid.de/custom-inference-huggingface-sagemaker)\n",
    "\n",
    "In addition to the `inference.py` file we also have to provide a `requirements.txt` file. The `requirements.txt` file is used to install the dependencies for our `inference.py` file.\n",
    "\n",
    "The first step is to create a `code/` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4246c06",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory â€˜codeâ€™: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435a574f",
   "metadata": {},
   "source": [
    "As next we create a `requirements.txt` file and add the `diffusers` library to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5bf302de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting code/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile code/requirements.txt\n",
    "diffusers>=0.6.0\n",
    "transformers>=4.23.1\n",
    "xformers>=0.0.13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24c2ca9",
   "metadata": {},
   "source": [
    "The last step for our inference handler is to create the `inference.py` file. The `inference.py` file is responsible for loading the model and handling the inference request. The `model_fn` function is called when the model is loaded. The `predict_fn` function is called when we want to do inference. \n",
    "\n",
    "We are using the `diffusers` library to load the model in the `model_fn` and generate 4 image for an input prompt with the `predict_fn`. The `predict_fn` function returns the `4` image as a `base64` encoded string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3ce41529",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting code/inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile code/inference.py\n",
    "import base64\n",
    "import torch\n",
    "from io import BytesIO\n",
    "from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\n",
    "\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    # Load stable diffusion and move it to the GPU\n",
    "    scheduler = EulerDiscreteScheduler.from_pretrained(model_dir, subfolder=\"scheduler\")\n",
    "    pipe = StableDiffusionPipeline.from_pretrained(model_dir, \n",
    "                                                   scheduler=scheduler,\n",
    "                                                   revision=\"fp16\",\n",
    "                                                   torch_dtype=torch.float16)\n",
    "    pipe = pipe.to(\"cuda\")\n",
    "    pipe.enable_xformers_memory_efficient_attention()\n",
    "\n",
    "    return pipe\n",
    "\n",
    "\n",
    "def predict_fn(data, pipe):\n",
    "\n",
    "    # get prompt & parameters\n",
    "    prompt = data.pop(\"inputs\", data)\n",
    "    # set valid HP for stable diffusion\n",
    "    num_inference_steps = data.pop(\"num_inference_steps\", 25)\n",
    "    guidance_scale = data.pop(\"guidance_scale\", 6.5)\n",
    "    num_images_per_prompt = data.pop(\"num_images_per_prompt\", 2)\n",
    "    image_length = data.pop(\"image_length\", 512)\n",
    "\n",
    "    # run generation with parameters\n",
    "    generated_images = pipe(\n",
    "        prompt,\n",
    "        num_inference_steps=num_inference_steps,\n",
    "        guidance_scale=guidance_scale,\n",
    "        num_images_per_prompt=num_images_per_prompt,\n",
    "        height=image_length,\n",
    "        width=image_length,\n",
    "    #)[\"images\"] # for Stabel Diffusion v1.x\n",
    "    ).images\n",
    "\n",
    "    # create response\n",
    "    encoded_images = []\n",
    "    for image in generated_images:\n",
    "        buffered = BytesIO()\n",
    "        image.save(buffered, format=\"JPEG\")\n",
    "        encoded_images.append(base64.b64encode(buffered.getvalue()).decode())\n",
    "\n",
    "    # create response\n",
    "    return {\"generated_images\": encoded_images}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144d8ccb",
   "metadata": {},
   "source": [
    "## Create SageMaker `model.tar.gz` artifact\n",
    "\n",
    "To use our `inference.py` we need to bundle it together with our model weights into a `model.tar.gz`. The archive includes all our model-artifcats to run inference. The `inference.py` script will be placed into a `code/` folder. We will use the `huggingface_hub` SDK to easily download [CompVis/stable-diffusion-v1-4](https://huggingface.co/CompVis/stable-diffusion-v1-4) from [Hugging Face](https://huggingface.co/CompVis/stable-diffusion-v1-4) and then upload it to Amazon S3 with the `sagemaker` SDK.\n",
    "\n",
    "Before we can load our model from the Hugging Face Hub we have to make sure that we accepted the license of [CompVis/stable-diffusion-v1-4](https://huggingface.co/CompVis/stable-diffusion-v1-4) to be able to use it. [CompVis/stable-diffusion-v1-4](https://huggingface.co/CompVis/stable-diffusion-v1-4) is published under the [CreativeML OpenRAIL-M license](https://huggingface.co/spaces/CompVis/stable-diffusion-license). You can accept the license by clicking on the `Agree and access repository` button on the model page at: https://huggingface.co/CompVis/stable-diffusion-v1-4. \n",
    "\n",
    "![license](https://raw.githubusercontent.com/huggingface/notebooks/main/sagemaker/23_stable_diffusion_inference/imgs/license.png)\n",
    "\n",
    "_Note: This will give access to the repository for the logged in user. This user can then be used to generate [HF Tokens](https://huggingface.co/settings/tokens) to load the model programmatically._\n",
    "\n",
    "\n",
    "Before we can load the model make sure you have a valid [HF Token](https://huggingface.co/settings/token). You can create a token by going to your [Hugging Face Settings](https://huggingface.co/settings/token) and clicking on the `New token` button. Make sure the enviornment has enough diskspace to store the model, ~30GB should be enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5105d2dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "740144ac9c164dbaaf9d3a3929fb6c61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 16 files:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['model-6275/v2-1_768-nonema-pruned.ckpt',\n",
       " 'model-6275/model_index.json',\n",
       " 'model-6275/text_encoder/pytorch_model.bin',\n",
       " 'model-6275/text_encoder/config.json',\n",
       " 'model-6275/vae/diffusion_pytorch_model.bin',\n",
       " 'model-6275/vae/config.json',\n",
       " 'model-6275/unet/diffusion_pytorch_model.bin',\n",
       " 'model-6275/unet/config.json',\n",
       " 'model-6275/.gitattributes',\n",
       " 'model-6275/v2-1_768-ema-pruned.ckpt',\n",
       " 'model-6275/README.md',\n",
       " 'model-6275/tokenizer/tokenizer_config.json',\n",
       " 'model-6275/tokenizer/merges.txt',\n",
       " 'model-6275/tokenizer/special_tokens_map.json',\n",
       " 'model-6275/tokenizer/vocab.json',\n",
       " 'model-6275/scheduler/scheduler_config.json']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from distutils.dir_util import copy_tree\n",
    "from pathlib import Path\n",
    "from huggingface_hub import snapshot_download\n",
    "import random\n",
    "\n",
    "HF_MODEL_ID=\"stabilityai/stable-diffusion-2-1\"\n",
    "#HF_MODEL_ID=\"CompVis/stable-diffusion-v1-4\"\n",
    "HF_TOKEN=\"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\" # your hf token: https://huggingface.co/settings/tokens\n",
    "assert len(HF_TOKEN) > 0, \"Please set HF_TOKEN to your huggingface token. You can find it here: https://huggingface.co/settings/tokens\"\n",
    "\n",
    "# download snapshot\n",
    "snapshot_dir = snapshot_download(repo_id=HF_MODEL_ID,revision=\"fp16\",use_auth_token=HF_TOKEN)\n",
    "\n",
    "# create model dir\n",
    "model_tar = Path(f\"model-{random.getrandbits(16)}\")\n",
    "model_tar.mkdir(exist_ok=True)\n",
    "\n",
    "# copy snapshot to model dir\n",
    "copy_tree(snapshot_dir, str(model_tar))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bf2a0d",
   "metadata": {},
   "source": [
    "The next step is to copy the `code/` directory into the `model/` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "93240d00",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model-6275/code/requirements.txt', 'model-6275/code/inference.py']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# copy code/ to model dir\n",
    "copy_tree(\"code/\", str(model_tar.joinpath(\"code\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6080d38a",
   "metadata": {},
   "source": [
    "Before we can upload the model to Amazon S3 we have to create a `model.tar.gz` archive. Important is that the archive should directly contain all files and not a folder with the files. For example, your file should look like this:\n",
    "\n",
    "```\n",
    "model.tar.gz/\n",
    "|- model_index.json\n",
    "|- unet/\n",
    "|- code/\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ffac2aed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v2-1_768-nonema-pruned.ckpt\n",
      "model_index.json\n",
      "text_encoder\n",
      "vae\n",
      "unet\n",
      ".gitattributes\n",
      "v2-1_768-ema-pruned.ckpt\n",
      "README.md\n",
      "code\n",
      "tokenizer\n",
      "scheduler\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "import os\n",
    "\n",
    "# helper to create the model.tar.gz\n",
    "def compress(tar_dir=None,output_file=\"model.tar.gz\"):\n",
    "    parent_dir=os.getcwd()\n",
    "    os.chdir(tar_dir)\n",
    "    with tarfile.open(os.path.join(parent_dir, output_file), \"w:gz\") as tar:\n",
    "        for item in os.listdir('.'):\n",
    "          print(item)\n",
    "          tar.add(item, arcname=item)    \n",
    "    os.chdir(parent_dir)\n",
    "            \n",
    "compress(str(model_tar))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbce144",
   "metadata": {},
   "source": [
    "After we created the `model.tar.gz` archive we can upload it to Amazon S3. We will use the `sagemaker` SDK to upload the model to our sagemaker session bucket. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "175511f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model uploaded to: s3://sagemaker-us-east-1-415275363822/stabilityai/stable-diffusion-2-1/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "# upload model.tar.gz to s3\n",
    "s3_model_uri=S3Uploader.upload(local_path=\"model.tar.gz\", desired_s3_uri=f\"s3://{sess.default_bucket()}/{HF_MODEL_ID}\")\n",
    "\n",
    "print(f\"model uploaded to: {s3_model_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a146346",
   "metadata": {},
   "source": [
    "## Deploy the model to Amazon SageMaker\n",
    "\n",
    "After we have uploaded our model archive we can deploy our model to Amazon SageMaker. We will use `HuggingfaceModel` to create our real-time inference endpoint.\n",
    "\n",
    "We are going to deploy the model to an `g4dn.xlarge` instance. The `g4dn.xlarge` instance is a GPU instance with 1 NVIDIA Tesla T4 GPUs. If you are interested in how you could add autoscaling to your endpoint you can check out [Going Production: Auto-scaling Hugging Face Transformers with Amazon SageMaker](https://www.philschmid.de/auto-scaling-sagemaker-huggingface)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "96cb4aa3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------!"
     ]
    }
   ],
   "source": [
    "from sagemaker.huggingface.model import HuggingFaceModel\n",
    "\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "   model_data=s3_model_uri,      # path to your model and script\n",
    "   role=role,                    # iam role with permissions to create an Endpoint\n",
    "   transformers_version=\"4.17\",  # transformers version used\n",
    "   pytorch_version=\"1.10\",       # pytorch version used\n",
    "   py_version='py38',            # python version used\n",
    ")\n",
    "\n",
    "# deploy the endpoint endpoint\n",
    "predictor = huggingface_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.g4dn.xlarge\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b3812f",
   "metadata": {},
   "source": [
    "## Generate images using the deployed model\n",
    "\n",
    "The `.deploy()` returns an `HuggingFacePredictor` object which can be used to request inference. Our endpoint expects a `json` with at least `inputs` key. The `inputs` key is the input prompt for the model, which will be used to generate the image. Additionally, we can provide `num_inference_steps`, `guidance_scale` & `num_images_per_prompt` to controll the generation.\n",
    "\n",
    "The `predictor.predict()` function returns a `json` with the `generated_images` key. The `generated_images` key contains the `4` generated images as a `base64` encoded string. To decode our response we added a small helper function `decode_base64_to_image` which takes the `base64` encoded string and returns a `PIL.Image` object and `display_images`, which takes a list of `PIL.Image` objects and displays them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "51c5366b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from IPython.display import display\n",
    "import base64\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# helper decoder\n",
    "def decode_base64_image(image_string):\n",
    "  base64_image = base64.b64decode(image_string)\n",
    "  buffer = BytesIO(base64_image)\n",
    "  return Image.open(buffer)\n",
    "\n",
    "# display PIL images as grid\n",
    "def display_images(images=None,columns=3, width=100, height=100):\n",
    "    plt.figure(figsize=(width, height))\n",
    "    for i, image in enumerate(images):\n",
    "        plt.subplot(int(len(images) / columns + 1), columns, i + 1)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71647be",
   "metadata": {},
   "source": [
    "Now, lets generate some images. As example lets generate `3` images for the prompt `A dog trying catch a flying pizza art drawn by disney concept artists`. Generating `3` images takes around `30` seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d253d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_images_per_prompt = 2\n",
    "#prompt = \"A dog trying catch a flying pizza art drawn by disney concept artists, golden colour, high quality, highly detailed, elegant, sharp focus\"\n",
    "prompt = \"A female super-model posees on stage dressing a casual long vacation skirt, striped with color, photorealistic, high quality, highly detailed, elegant, sharp focus\"\n",
    "# run prediction\n",
    "response = predictor.predict(data={\n",
    "  \"inputs\": prompt,\n",
    "  \"num_images_per_prompt\" : num_images_per_prompt,\n",
    "  \"image_length\": 768\n",
    "  }\n",
    ")\n",
    "\n",
    "# decode images\n",
    "decoded_images = [decode_base64_image(image) for image in response[\"generated_images\"]]\n",
    "\n",
    "# visualize generation\n",
    "display_images(decoded_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb10007d",
   "metadata": {},
   "source": [
    "### Delete model and endpoint\n",
    "\n",
    "To clean up, we can delete the model and endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1e6fb7b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "One or more models cannot be deleted, please retry. \nFailed models: huggingface-pytorch-inference-2023-02-08-19-39-52-422",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-c3964cde68a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sagemaker/predictor.py\u001b[0m in \u001b[0;36mdelete_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrequest_failed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 348\u001b[0;31m             raise Exception(\n\u001b[0m\u001b[1;32m    349\u001b[0m                 \u001b[0;34m\"One or more models cannot be deleted, please retry. \\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m                 \u001b[0;34m\"Failed models: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\", \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfailed_models\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: One or more models cannot be deleted, please retry. \nFailed models: huggingface-pytorch-inference-2023-02-08-19-39-52-422"
     ]
    }
   ],
   "source": [
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11652666-817e-442c-8435-7a853a3a383f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.12 Python 3.8 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-1.12-gpu-py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "f6dd96c16031089903d5a31ec148b80aeb0d39c32affb1a1080393235fbfa2fc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
