{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2919083b",
   "metadata": {},
   "source": [
    "# Develop A Clinical Image Classifier From Pre-Trained Vision Transformers Models Using SageMaker Training Compiler "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0b1daf",
   "metadata": {},
   "source": [
    "1. [Introduction](#Introduction)  \n",
    "2. [Development Environment and Permissions](#Development-Environment-and-Permissions)\n",
    "    1. [Installation](#Installation)  \n",
    "    2. [SageMaker environment](#SageMaker-environment)\n",
    "3. [Working with the Caltech-256 dataset](#Working-with-the-Caltech-256-dataset)   \n",
    "4. [SageMaker Training Job](#SageMaker-Training-Job)  \n",
    "    1. [Training Setup](#Training-Setup)  \n",
    "    2. [Training with Native TensorFlow](#Training-with-Native-TensorFlow)  \n",
    "    3. [Training with Optimized TensorFlow](#Training-with-Optimized-TensorFlow)  \n",
    "5. [Analysis](#Analysis)\n",
    "    1. [Savings from Training Compiler](#Savings-from-Training-Compiler)\n",
    "    2. [Convergence of Training](#Convergence-of-Training)\n",
    "6. [Clean up](#Clean-up)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2656bd8",
   "metadata": {},
   "source": [
    "## SageMaker Training Compiler Overview\n",
    "\n",
    "SageMaker Training Compiler is a capability of SageMaker that makes hard-to-implement optimizations to reduce training time on GPU instances. The compiler optimizes Deep Learning (DL) models to accelerate training by more efficiently using SageMaker machine learning (ML) GPU instances. SageMaker Training Compiler is available at no additional charge within SageMaker and can help reduce total billable time as it accelerates training. \n",
    "\n",
    "SageMaker Training Compiler is integrated into the AWS Deep Learning Containers (DLCs). Using the SageMaker Training Compiler enabled AWS DLCs, you can compile and optimize training jobs on GPU instances with minimal changes to your code. Bring your deep learning models to SageMaker and enable SageMaker Training Compiler to accelerate the speed of your training job on SageMaker ML instances for accelerated computing. \n",
    "\n",
    "For more information, see [SageMaker Training Compiler](https://docs.aws.amazon.com/sagemaker/latest/dg/training-compiler.html) in the *Amazon SageMaker Developer Guide*.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Self-attention based Transformers architecture allows large models such as RoBERT and GPT-J deliver state-of-the-art performances in the natural language processing (NLP). Tranformers architecture didn’t initially benefit Computer Vision (CV) due to the complexity related to the application of the attention mecanism on large scale images. Most of the previous efforts make the models before Vision Transformers (ViT) almost unusable when the images dimensions need to be relatively important.\n",
    "\n",
    "ViT changes the CV paradiam by solving the dimension issue with splitting the image into patches of fixed sizes, which then are transformed to 1D embeddings or pass into a CNN to extract patches features maps. ViT has been proven to be more aware about the global context than the previous architecture like ResNet. CV tasks including image semantic segmentastion based on ViT or itsvariants such as Swin Transformers have achieved new level of performance results. [https://paperswithcode.com/sota/semantic-segmentation-on-ade20k]  \n",
    "\n",
    "![Image](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/swin_transformer_architecture.png)\n",
    "\n",
    "Figure 1. Comparing ViT with Swin Transformers Feature Maps (Curtsey of https://arxiv.org/pdf/2103.14030.pdf)  \n",
    "\n",
    "In comparison to CNN archotecture, the ViT’s weaker inductive bias is generally found to cause an increased reliance on model regularization or data augmentation (AugReg) when training on smaller training datasets. ViT models are generally found to perform best in settings with large amounts of training datasets [https://arxiv.org/abs/2010.11929] or to require strong AugReg schemes to avoid overfitting [https://arxiv.org/abs/2012.12877]. Steiner et al [https://www.arxiv-vanity.com/papers/2106.10270/]comprehensive conducted a comprehensive study of the trade-offs between model regularization, data augmentation, training data size and compute budget in Vision Transformers and found that the combination of increased compute and AugReg can yield models with the same performance as models trained on an order of magnitude more training data. They have trained ViT models of various sizes on the public ImageNet-21k dataset which either match or outperform their counterparts trained on the larger.\n",
    "\n",
    "The notebook walk you through how to leverage the pre-train ViT models with different sizes and hybrids with ResNets on ImageNet21K and apply transfer learning on medical images for clinic diagnotics classification. We focus mainly on the perspective of an use case with limited compute resources and data annotation budgets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffd8e1f",
   "metadata": {},
   "source": [
    "## Solution Overiew\n",
    "\n",
    "In this blog, you'll use Amazon SageMaker to train an Image Classifier model via transfer learning based on the pre-trained ViT to help endoscopists to automate detection of various anatomical landmarks, phatological findings or endoscopic procedures in the gastrointestinal tract. The botebook sample swill cover the following contents to assist machine learning developers to quickly get started with creating domain specific Imager Classifier with simplecity and less computational costs:\n",
    "\n",
    "* Installing prerequisites\n",
    "* Downloading the Kvasir V2 dataset ~2.3 GB and Loading It\n",
    "* Choosing an Image Classifier model on HuggingFace\n",
    "* Setting-up the Trainer and start the Fine-Tuning\n",
    "* Leverage SageMaker training acceleration features\n",
    "* Evaluating the Performance of the Model\n",
    "* Using HuggingFace to Run Inference on images\n",
    "* Conclusion & Citations\n",
    "\n",
    "**NOTE:** You can run this demo in SageMaker Studio, SageMaker notebook instances, or your local machine with AWS CLI set up. If using SageMaker Studio or SageMaker notebook instances, make sure you choose one of the TensorFlow-based kernels, `Python 3 (TensorFlow x.y Python 3.x CPU Optimized)` or `conda_tensorflow_p39` respectively.\n",
    "\n",
    "**NOTE:** This notebook uses a `ml.p3.2xlarge` instance with a single GPU. However, it can easily be extended to multiple GPUs on a single node. If you don't have enough quota, see [Request a service quota increase for SageMaker resources](https://docs.aws.amazon.com/sagemaker/latest/dg/regions-quotas.html#service-limit-increase-request-procedure). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e4ebe1",
   "metadata": {},
   "source": [
    "## Prerequisities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96d1125",
   "metadata": {},
   "source": [
    "### Installation\n",
    "\n",
    "This example notebook requires **SageMaker Python SDK v2.95.0 or later**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f2da8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install \"sagemaker>=2.95\" botocore boto3 awscli matplotlib --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "126ef8e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "botocore: 1.27.15\n",
      "boto3: 1.24.15\n",
      "sagemaker: 2.96.0\n"
     ]
    }
   ],
   "source": [
    "import botocore\n",
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "print(f\"botocore: {botocore.__version__}\")\n",
    "print(f\"boto3: {boto3.__version__}\")\n",
    "print(f\"sagemaker: {sagemaker.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7670563b",
   "metadata": {},
   "source": [
    "### SageMaker environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "299964a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::976939723775:role/service-role/AmazonSageMaker-ExecutionRole-20210317T133000\n",
      "sagemaker bucket: sagemaker-us-west-2-976939723775\n",
      "sagemaker session region: us-west-2\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "# SageMaker session bucket -> used for uploading data, models and logs\n",
    "# SageMaker will automatically create this bucket if it does not exist\n",
    "sagemaker_session_bucket = None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sagemaker_session_bucket}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee617a5",
   "metadata": {},
   "source": [
    "## Working with the Caltech-256 dataset\n",
    "\n",
    "We have hosted the [Caltech-256](https://authors.library.caltech.edu/7694/) dataset in S3 in us-east-1. We will transfer this dataset to your account and region for use with SageMaker Training.\n",
    "\n",
    "The dataset consists of JPEG images organized into directories with each directory representing an object category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "769b0628",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "source = \"s3://sagemaker-sample-files/datasets/image/caltech-256/256_ObjectCategories\"\n",
    "destn = f\"s3://{sagemaker_session_bucket}/public_datasets/caltech-256/\"\n",
    "local = \"caltech-256\"\n",
    "\n",
    "#os.system(f\"aws s3 sync {source} {local}\")\n",
    "#os.system(f\"aws s3 sync {local} {destn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ace828e",
   "metadata": {},
   "source": [
    "## SageMaker Training Job\n",
    "\n",
    "To create a SageMaker training job, we use a `TensorFlow` estimator. Using the estimator, you can define which training script should SageMaker use through `entry_point`, which `instance_type` to use for training, which `hyperparameters` to pass, and so on.\n",
    "\n",
    "When a SageMaker training job starts, SageMaker takes care of starting and managing all the required machine learning instances, picks up the `TensorFlow` Deep Learning Container, uploads your training script, and downloads the data from `sagemaker_session_bucket` into the container at `/opt/ml/input/data`.\n",
    "\n",
    "In the following section, you learn how to set up two versions of the SageMaker `TensorFlow` estimator, a native one without the compiler and an optimized one with the compiler."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5823ac48",
   "metadata": {},
   "source": [
    "### Training with Native TensorFlow\n",
    "\n",
    "The `BATCH_SIZE` in the following code cell is the maximum batch that can fit into the memory of a `ml.p3.2xlarge` instance while giving the best training speed. If you change the model, instance type, and other parameters, you need to do some experiments to find the largest batch size that will fit into GPU memory.\n",
    "\n",
    "Set `EPOCHS` to the number of times you would like to loop over the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "debfd234",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-14 17:44:17 Starting - Starting the training job...\n",
      "2023-02-14 17:44:42 Starting - Preparing the instances for training......\n",
      "2023-02-14 17:45:43 Downloading - Downloading input data......\n",
      "2023-02-14 17:46:28 Training - Downloading the training image............\n",
      "2023-02-14 17:48:44 Training - Training image download completed. Training in progress....\u001b[34m2023-02-14 17:49:02.637180: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m2023-02-14 17:49:02.637359: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m2023-02-14 17:49:02.681676: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m2023-02-14 17:49:05,157 sagemaker-training-toolkit INFO     Imported framework sagemaker_tensorflow_container.training\u001b[0m\n",
      "\u001b[34m2023-02-14 17:49:05,412 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/usr/local/bin/python3.9 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpi4py>=3.1.3 in /usr/local/lib/python3.9/site-packages (from -r requirements.txt (line 2)) (3.1.3)\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip available: 22.1.2 -> 23.0\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2023-02-14 17:49:07,952 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-02-14 17:49:07,953 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-02-14 17:49:08,065 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p3.2xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_tensorflow_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"BATCH_SIZE\": 64,\n",
      "        \"EPOCHS\": 10,\n",
      "        \"LEARNING_RATE\": 0.001,\n",
      "        \"WEIGHT_DECAY\": 0.0001\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p3.2xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"native-tf29-vit-2023-02-14-17-44-16-764\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-976939723775/native-tf29-vit-2023-02-14-17-44-16-764/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"vit\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p3.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p3.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"vit.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"BATCH_SIZE\":64,\"EPOCHS\":10,\"LEARNING_RATE\":0.001,\"WEIGHT_DECAY\":0.0001}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=vit.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.p3.2xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=vit\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-west-2-976939723775/native-tf29-vit-2023-02-14-17-44-16-764/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.p3.2xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_tensorflow_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"BATCH_SIZE\":64,\"EPOCHS\":10,\"LEARNING_RATE\":0.001,\"WEIGHT_DECAY\":0.0001},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"job_name\":\"native-tf29-vit-2023-02-14-17-44-16-764\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-976939723775/native-tf29-vit-2023-02-14-17-44-16-764/source/sourcedir.tar.gz\",\"module_name\":\"vit\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"vit.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--BATCH_SIZE\",\"64\",\"--EPOCHS\",\"10\",\"--LEARNING_RATE\",\"0.001\",\"--WEIGHT_DECAY\",\"0.0001\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH_SIZE=64\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=10\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=0.001\u001b[0m\n",
      "\u001b[34mSM_HP_WEIGHT_DECAY=0.0001\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/local/lib/python39.zip:/usr/local/lib/python3.9:/usr/local/lib/python3.9/lib-dynload:/usr/local/lib/python3.9/site-packages:/usr/local/lib/python3.9/site-packages/smdebug-1.0.17b20220701-py3.9.egg:/usr/local/lib/python3.9/site-packages/pyinstrument-3.4.2-py3.9.egg:/usr/local/lib/python3.9/site-packages/pyinstrument_cext-0.2.4-py3.9-linux-x86_64.egg\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/usr/local/bin/python3.9 vit.py --BATCH_SIZE 64 --EPOCHS 10 --LEARNING_RATE 0.001 --WEIGHT_DECAY 0.0001\u001b[0m\n",
      "\u001b[34m2023-02-14 17:49:08.677045: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m2023-02-14 17:49:08.677207: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m2023-02-14 17:49:08.725545: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34mTraining on Images of size (224, 224, 3)\u001b[0m\n",
      "\u001b[34mTraining on dataset size (18297, 224, 224, 3)\u001b[0m\n",
      "\u001b[34mExtension horovod.torch has not been built: /usr/local/lib/python3.9/site-packages/horovod/torch/mpi_lib/_mpi_lib.cpython-39-x86_64-linux-gnu.so not found\u001b[0m\n",
      "\u001b[34mIf this is not expected, reinstall Horovod with HOROVOD_WITH_PYTORCH=1 to debug the build error.\u001b[0m\n",
      "\u001b[34mWarning! MPI libs are missing, but python applications are still available.\u001b[0m\n",
      "\u001b[34m[2023-02-14 17:51:39.232 ip-10-0-222-234.us-west-2.compute.internal:42 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.9/site-packages/smdebug-1.0.17b20220701-py3.9.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.9/site-packages/smdebug-1.0.17b20220701-py3.9.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[2023-02-14 17:51:39.509 ip-10-0-222-234.us-west-2.compute.internal:42 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[2023-02-14 17:51:39.537 ip-10-0-222-234.us-west-2.compute.internal:42 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2023-02-14 17:51:39.537 ip-10-0-222-234.us-west-2.compute.internal:42 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2023-02-14 17:51:39.538 ip-10-0-222-234.us-west-2.compute.internal:42 INFO hook.py:254] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2023-02-14 17:51:39.538 ip-10-0-222-234.us-west-2.compute.internal:42 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2023-02-14 17:51:39.538 ip-10-0-222-234.us-west-2.compute.internal:42 INFO hook.py:421] Monitoring the collections: sm_metrics, losses, metrics\u001b[0m\n",
      "\u001b[34mEpoch 1/10\u001b[0m\n",
      "\u001b[34m286/286 - 141s - loss: 5.8124 - accuracy: 0.0192 - top-5-accuracy: 0.0594 - 141s/epoch - 494ms/step\u001b[0m\n",
      "\u001b[34mEpoch 2/10\u001b[0m\n",
      "\u001b[34m286/286 - 114s - loss: 5.7565 - accuracy: 0.0220 - top-5-accuracy: 0.0620 - 114s/epoch - 398ms/step\u001b[0m\n",
      "\u001b[34mEpoch 3/10\u001b[0m\n",
      "\u001b[34m286/286 - 114s - loss: 5.7529 - accuracy: 0.0212 - top-5-accuracy: 0.0624 - 114s/epoch - 398ms/step\u001b[0m\n",
      "\u001b[34mEpoch 4/10\u001b[0m\n",
      "\u001b[34m286/286 - 114s - loss: 5.7613 - accuracy: 0.0205 - top-5-accuracy: 0.0639 - 114s/epoch - 398ms/step\u001b[0m\n",
      "\u001b[34mEpoch 5/10\u001b[0m\n",
      "\u001b[34m286/286 - 114s - loss: 5.7727 - accuracy: 0.0206 - top-5-accuracy: 0.0625 - 114s/epoch - 397ms/step\u001b[0m\n",
      "\u001b[34mEpoch 6/10\u001b[0m\n",
      "\u001b[34m286/286 - 114s - loss: 5.7547 - accuracy: 0.0206 - top-5-accuracy: 0.0634 - 114s/epoch - 397ms/step\u001b[0m\n",
      "\u001b[34mEpoch 7/10\u001b[0m\n",
      "\u001b[34m286/286 - 114s - loss: 5.7580 - accuracy: 0.0208 - top-5-accuracy: 0.0636 - 114s/epoch - 397ms/step\u001b[0m\n",
      "\u001b[34mEpoch 8/10\u001b[0m\n",
      "\u001b[34m286/286 - 114s - loss: 5.7323 - accuracy: 0.0222 - top-5-accuracy: 0.0650 - 114s/epoch - 397ms/step\u001b[0m\n",
      "\u001b[34mEpoch 9/10\u001b[0m\n",
      "\u001b[34m286/286 - 114s - loss: 5.7478 - accuracy: 0.0209 - top-5-accuracy: 0.0646 - 114s/epoch - 398ms/step\u001b[0m\n",
      "\u001b[34mEpoch 10/10\u001b[0m\n",
      "\u001b[34m286/286 - 114s - loss: 5.7533 - accuracy: 0.0219 - top-5-accuracy: 0.0628 - 114s/epoch - 397ms/step\u001b[0m\n",
      "\u001b[34m2023-02-14 18:11:12,650 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-02-14 18:11:12,650 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-02-14 18:11:12,651 sagemaker_tensorflow_container.training WARNING  No model artifact is saved under path /opt/ml/model. Your training job will not save any model files to S3.\u001b[0m\n",
      "\u001b[34mFor details of how to construct your training script see:\u001b[0m\n",
      "\u001b[34mhttps://sagemaker.readthedocs.io/en/stable/using_tf.html#adapting-your-local-tensorflow-script\u001b[0m\n",
      "\u001b[34m2023-02-14 18:11:12,651 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2023-02-14 18:11:26 Uploading - Uploading generated training model\n",
      "2023-02-14 18:11:26 Completed - Training job completed\n",
      "Training seconds: 1543\n",
      "Billable seconds: 1543\n",
      "CPU times: user 3.37 s, sys: 154 ms, total: 3.53 s\n",
      "Wall time: 27min 27s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'native-tf29-vit-2023-02-14-17-44-16-764'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "\n",
    "env={'SAGEMAKER_REQUIREMENTS': 'requirements.txt'}\n",
    "\n",
    "kwargs = dict(\n",
    "    source_dir=\"scripts\",\n",
    "    entry_point=\"vit.py\",\n",
    "    model_dir=False,\n",
    "    env=env,\n",
    "    instance_type=\"ml.p3.2xlarge\",\n",
    "    instance_count=1,\n",
    "    framework_version=\"2.9.1\", #original 2.9.1\n",
    "    py_version=\"py39\", #original 3.9\n",
    "    debugger_hook_config=None,\n",
    "    disable_profiler=True,\n",
    "    max_run=60 * 60,  # 60 minutes\n",
    "    role=role,\n",
    "    metric_definitions=[\n",
    "        {\"Name\": \"training_loss\", \"Regex\": \"loss: ([0-9.]*?) \"},\n",
    "        {\"Name\": \"training_accuracy\", \"Regex\": \"accuracy: ([0-9.]*?) \"},\n",
    "        {\"Name\": \"training_latency_per_epoch\", \"Regex\": \"- ([0-9.]*?)s/epoch\"},\n",
    "        {\"Name\": \"training_avg_latency_per_step\", \"Regex\": \"- ([0-9.]*?)ms/step\"},\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Configure the training job\n",
    "native_estimator = TensorFlow(\n",
    "    hyperparameters={\n",
    "        \"EPOCHS\": EPOCHS,\n",
    "        \"BATCH_SIZE\": BATCH_SIZE,\n",
    "        \"LEARNING_RATE\": LEARNING_RATE,\n",
    "        \"WEIGHT_DECAY\": WEIGHT_DECAY,\n",
    "    },\n",
    "    base_job_name=\"native-tf29-vit\",\n",
    "    **kwargs,\n",
    ")\n",
    "\n",
    "# Start training with our uploaded datasets as input\n",
    "native_estimator.fit(inputs=destn, wait=True)\n",
    "\n",
    "# The name of the training job.\n",
    "native_estimator.latest_training_job.name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfc9a99",
   "metadata": {},
   "source": [
    "### Training with Optimized TensorFlow\n",
    "\n",
    "Compilation through Training Compiler changes the memory footprint of the model. Most commonly, this manifests as a reduction in memory utilization and a consequent increase in the largest batch size that can fit on the GPU. But in some cases the compiler intelligently promotes caching which leads to a decrease in the largest batch size that can fit on the GPU. Note that if you want to change the batch size, you must adjust the learning rate appropriately.\n",
    "\n",
    "**Note:** We recommend you to turn the SageMaker Debugger's profiling and debugging tools off when you use compilation to avoid additional overheads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "374e3f5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'optimized-tf29-vit-2023-02-14-18-11-44-149'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker.tensorflow import TensorFlow, TrainingCompilerConfig\n",
    "\n",
    "OPTIMIZED_BATCH_SIZE = 48\n",
    "LEARNING_RATE = LEARNING_RATE / BATCH_SIZE * OPTIMIZED_BATCH_SIZE\n",
    "WEIGHT_DECAY = WEIGHT_DECAY * BATCH_SIZE / OPTIMIZED_BATCH_SIZE\n",
    "\n",
    "# Configure the training job\n",
    "optimized_estimator = TensorFlow(\n",
    "    hyperparameters={\n",
    "        \"EPOCHS\": EPOCHS,\n",
    "        \"BATCH_SIZE\": OPTIMIZED_BATCH_SIZE,\n",
    "        \"LEARNING_RATE\": LEARNING_RATE,\n",
    "        \"WEIGHT_DECAY\": WEIGHT_DECAY,\n",
    "    },\n",
    "    compiler_config=TrainingCompilerConfig(),\n",
    "    base_job_name=\"optimized-tf29-vit\",\n",
    "    **kwargs,\n",
    ")\n",
    "\n",
    "# Start training with our uploaded datasets as input\n",
    "optimized_estimator.fit(inputs=destn, wait=False)\n",
    "\n",
    "# The name of the training job.\n",
    "optimized_estimator.latest_training_job.name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c60155",
   "metadata": {},
   "source": [
    "### Wait for training jobs to complete\n",
    "\n",
    "The training jobs described above typically take around 40 mins to complete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08b9df1",
   "metadata": {},
   "source": [
    "**Note:** If the estimator object is no longer available due to a kernel break or refresh, you need to directly use the training job name and manually attach the training job to a new TensorFlow estimator. For example:\n",
    "\n",
    "```python\n",
    "native_estimator = TensorFlow.attach(\"<your_training_job_name>\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a456239f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "waiter = sess.sagemaker_client.get_waiter(\"training_job_completed_or_stopped\")\n",
    "\n",
    "waiter.wait(TrainingJobName=native_estimator.latest_training_job.name)\n",
    "waiter.wait(TrainingJobName=optimized_estimator.latest_training_job.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "094dc24c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2023-02-14 18:11:26 Starting - Preparing the instances for training\n",
      "2023-02-14 18:11:26 Downloading - Downloading input data\n",
      "2023-02-14 18:11:26 Training - Training image download completed. Training in progress.\n",
      "2023-02-14 18:11:26 Uploading - Uploading generated training model\n",
      "2023-02-14 18:11:26 Completed - Training job completed\n",
      "\n",
      "2023-02-14 18:32:22 Starting - Preparing the instances for training\n",
      "2023-02-14 18:32:22 Downloading - Downloading input data\n",
      "2023-02-14 18:32:22 Training - Training image download completed. Training in progress.\n",
      "2023-02-14 18:32:22 Uploading - Uploading generated training model\n",
      "2023-02-14 18:32:22 Completed - Training job completed\n"
     ]
    }
   ],
   "source": [
    "native_estimator = TensorFlow.attach(native_estimator.latest_training_job.name)\n",
    "optimized_estimator = TensorFlow.attach(optimized_estimator.latest_training_job.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59521483",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "Here we view the training metrics from the training jobs as a Pandas dataframe\n",
    "\n",
    "#### Native TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7182962",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>training_loss</th>\n",
       "      <th>training_accuracy</th>\n",
       "      <th>training_latency_per_epoch</th>\n",
       "      <th>training_avg_latency_per_step</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epochs</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.8124</td>\n",
       "      <td>0.0192</td>\n",
       "      <td>141.0</td>\n",
       "      <td>494.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.7565</td>\n",
       "      <td>0.0220</td>\n",
       "      <td>114.0</td>\n",
       "      <td>398.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.7529</td>\n",
       "      <td>0.0212</td>\n",
       "      <td>114.0</td>\n",
       "      <td>398.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.7613</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>114.0</td>\n",
       "      <td>398.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.7727</td>\n",
       "      <td>0.0206</td>\n",
       "      <td>114.0</td>\n",
       "      <td>397.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5.7547</td>\n",
       "      <td>0.0206</td>\n",
       "      <td>114.0</td>\n",
       "      <td>397.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.7580</td>\n",
       "      <td>0.0208</td>\n",
       "      <td>114.0</td>\n",
       "      <td>397.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5.7323</td>\n",
       "      <td>0.0222</td>\n",
       "      <td>114.0</td>\n",
       "      <td>397.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5.7478</td>\n",
       "      <td>0.0209</td>\n",
       "      <td>114.0</td>\n",
       "      <td>398.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5.7533</td>\n",
       "      <td>0.0219</td>\n",
       "      <td>114.0</td>\n",
       "      <td>397.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        training_loss  training_accuracy  training_latency_per_epoch  \\\n",
       "epochs                                                                 \n",
       "1              5.8124             0.0192                       141.0   \n",
       "2              5.7565             0.0220                       114.0   \n",
       "3              5.7529             0.0212                       114.0   \n",
       "4              5.7613             0.0205                       114.0   \n",
       "5              5.7727             0.0206                       114.0   \n",
       "6              5.7547             0.0206                       114.0   \n",
       "7              5.7580             0.0208                       114.0   \n",
       "8              5.7323             0.0222                       114.0   \n",
       "9              5.7478             0.0209                       114.0   \n",
       "10             5.7533             0.0219                       114.0   \n",
       "\n",
       "        training_avg_latency_per_step  \n",
       "epochs                                 \n",
       "1                               494.0  \n",
       "2                               398.0  \n",
       "3                               398.0  \n",
       "4                               398.0  \n",
       "5                               397.0  \n",
       "6                               397.0  \n",
       "7                               397.0  \n",
       "8                               397.0  \n",
       "9                               398.0  \n",
       "10                              397.0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Extract training metrics from the estimator\n",
    "native_metrics = native_estimator.training_job_analytics.dataframe()\n",
    "\n",
    "# Restructure table for viewing\n",
    "for metric in native_metrics[\"metric_name\"].unique():\n",
    "    native_metrics[metric] = native_metrics[native_metrics[\"metric_name\"] == metric][\"value\"]\n",
    "native_metrics = native_metrics.drop(columns=[\"metric_name\", \"value\"])\n",
    "native_metrics = native_metrics.groupby(\"timestamp\").max()\n",
    "native_metrics[\"epochs\"] = range(1, 11)\n",
    "native_metrics = native_metrics.set_index(\"epochs\")\n",
    "\n",
    "native_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091fe4e4",
   "metadata": {},
   "source": [
    "#### Optimized TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b2b6689",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>training_loss</th>\n",
       "      <th>training_accuracy</th>\n",
       "      <th>training_latency_per_epoch</th>\n",
       "      <th>training_avg_latency_per_step</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epochs</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.7546</td>\n",
       "      <td>0.0204</td>\n",
       "      <td>106.0</td>\n",
       "      <td>278.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.7167</td>\n",
       "      <td>0.0213</td>\n",
       "      <td>65.0</td>\n",
       "      <td>170.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.6898</td>\n",
       "      <td>0.0225</td>\n",
       "      <td>65.0</td>\n",
       "      <td>170.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.6865</td>\n",
       "      <td>0.0237</td>\n",
       "      <td>65.0</td>\n",
       "      <td>170.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.6771</td>\n",
       "      <td>0.0242</td>\n",
       "      <td>65.0</td>\n",
       "      <td>171.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5.4555</td>\n",
       "      <td>0.0284</td>\n",
       "      <td>65.0</td>\n",
       "      <td>170.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.3270</td>\n",
       "      <td>0.0347</td>\n",
       "      <td>65.0</td>\n",
       "      <td>170.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5.2181</td>\n",
       "      <td>0.0420</td>\n",
       "      <td>65.0</td>\n",
       "      <td>170.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5.0186</td>\n",
       "      <td>0.0529</td>\n",
       "      <td>65.0</td>\n",
       "      <td>170.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4.8606</td>\n",
       "      <td>0.0696</td>\n",
       "      <td>65.0</td>\n",
       "      <td>170.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        training_loss  training_accuracy  training_latency_per_epoch  \\\n",
       "epochs                                                                 \n",
       "1              5.7546             0.0204                       106.0   \n",
       "2              5.7167             0.0213                        65.0   \n",
       "3              5.6898             0.0225                        65.0   \n",
       "4              5.6865             0.0237                        65.0   \n",
       "5              5.6771             0.0242                        65.0   \n",
       "6              5.4555             0.0284                        65.0   \n",
       "7              5.3270             0.0347                        65.0   \n",
       "8              5.2181             0.0420                        65.0   \n",
       "9              5.0186             0.0529                        65.0   \n",
       "10             4.8606             0.0696                        65.0   \n",
       "\n",
       "        training_avg_latency_per_step  \n",
       "epochs                                 \n",
       "1                               278.0  \n",
       "2                               170.0  \n",
       "3                               170.0  \n",
       "4                               170.0  \n",
       "5                               171.0  \n",
       "6                               170.0  \n",
       "7                               170.0  \n",
       "8                               170.0  \n",
       "9                               170.0  \n",
       "10                              170.0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Extract training metrics from the estimator\n",
    "optimized_metrics = optimized_estimator.training_job_analytics.dataframe()\n",
    "\n",
    "# Restructure table for viewing\n",
    "for metric in optimized_metrics[\"metric_name\"].unique():\n",
    "    optimized_metrics[metric] = optimized_metrics[optimized_metrics[\"metric_name\"] == metric][\n",
    "        \"value\"\n",
    "    ]\n",
    "optimized_metrics = optimized_metrics.drop(columns=[\"metric_name\", \"value\"])\n",
    "optimized_metrics = optimized_metrics.groupby(\"timestamp\").max()\n",
    "optimized_metrics[\"epochs\"] = range(1, 11)\n",
    "optimized_metrics = optimized_metrics.set_index(\"epochs\")\n",
    "\n",
    "optimized_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8bd0a6",
   "metadata": {},
   "source": [
    "### Savings from Training Compiler\n",
    "\n",
    "Let us calculate the actual savings on the training jobs above and the potential for savings for a longer training job.\n",
    "\n",
    "#### Actual Savings\n",
    "\n",
    "To get the actual savings, we use the describe_training_job API to get the billable seconds for each training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4de73ff8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1543"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Billable seconds for the Native TensorFlow Training job\n",
    "\n",
    "details = sess.describe_training_job(job_name=native_estimator.latest_training_job.name)\n",
    "native_secs = details[\"BillableTimeInSeconds\"]\n",
    "\n",
    "native_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8072cb0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1157"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Billable seconds for the Optimized TensorFlow Training job\n",
    "\n",
    "details = sess.describe_training_job(job_name=optimized_estimator.latest_training_job.name)\n",
    "optimized_secs = details[\"BillableTimeInSeconds\"]\n",
    "\n",
    "optimized_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90be4082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Training Compiler yielded 25.02% savings in training cost.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating percentage Savings from Training Compiler\n",
    "\n",
    "percentage = (native_secs - optimized_secs) * 100 / native_secs\n",
    "\n",
    "f\"Training Compiler yielded {percentage:.2f}% savings in training cost.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34156d7d",
   "metadata": {},
   "source": [
    "#### Potential savings\n",
    "\n",
    "The Training Compiler works by compiling the model graph once per input shape and reusing the cached graph for subsequent steps. As a result the first few steps of training incur an increased latency owing to compilation which we refer to as the compilation overhead. This overhead is amortized over time thanks to the subsequent steps being much faster. We will demonstrate this below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d78280c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f33510108b0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(native_metrics[\"training_latency_per_epoch\"], label=\"native_epoch_latency\")\n",
    "plt.plot(optimized_metrics[\"training_latency_per_epoch\"], label=\"optimized_epoch_latency\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662640bd",
   "metadata": {},
   "source": [
    "We calculate the potential savings below from the difference in steady state epoch latency between native TensorFlow and optimized TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4eb14d57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "native_steady_state_latency = native_metrics[\"training_latency_per_epoch\"].iloc[-1]\n",
    "\n",
    "native_steady_state_latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "876bdde1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimized_steady_state_latency = optimized_metrics[\"training_latency_per_epoch\"].iloc[-1]\n",
    "\n",
    "optimized_steady_state_latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63c27f83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Training Compiler can potentially yield 42.98% savings in training cost for a longer training job.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating potential percentage Savings from Training Compiler\n",
    "\n",
    "percentage = (\n",
    "    (native_steady_state_latency - optimized_steady_state_latency)\n",
    "    * 100\n",
    "    / native_steady_state_latency\n",
    ")\n",
    "\n",
    "f\"Training Compiler can potentially yield {percentage:.2f}% savings in training cost for a longer training job.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a6d1ed",
   "metadata": {},
   "source": [
    "### Convergence of Training\n",
    "\n",
    "Training Compiler brings down total training time by intelligently choosing between memory utilization and core utilization in the GPU. This does not have any effect on the model arithmetic and consequently convergence of the model.\n",
    "\n",
    "However, since we are working with a new batch size, hyperparameters like - learning rate, learning rate schedule and weight decay might have to be scaled and tuned for the new batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bddf1424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f33514e4f40>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(native_metrics[\"training_loss\"], label=\"native_loss\")\n",
    "plt.plot(optimized_metrics[\"training_loss\"], label=\"optimized_loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5f157f",
   "metadata": {},
   "source": [
    "We can see that the model's convergence behavior is similar with and without Training Compiler. Here we have tuned the batch size specific hyperparameters - Learning Rate and Weight Decay using a linear scaling.\n",
    "\n",
    "Learning rate is directly proportional to the batch size:\n",
    "```python\n",
    "new_learning_rate = old_learning_rate * new_batch_size/old_batch_size\n",
    "```\n",
    "\n",
    "Weight decay is inversely proportional to the batch size:\n",
    "```python\n",
    "new_weight_decay = old_weight_decay * old_batch_size/new_batch_size\n",
    "```\n",
    "\n",
    "Better results can be achieved with further tuning. Check out [Automatic Model Tuning](https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning.html) for tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af69983",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "\n",
    "Stop all training jobs launched if the jobs are still running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf452ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_training_job(name):\n",
    "    status = sess.describe_training_job(name)[\"TrainingJobStatus\"]\n",
    "    if status == \"InProgress\":\n",
    "        sm.stop_training_job(TrainingJobName=name)\n",
    "\n",
    "\n",
    "stop_training_job(native_estimator.latest_training_job.name)\n",
    "stop_training_job(optimized_estimator.latest_training_job.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8f2fdf",
   "metadata": {},
   "source": [
    "Also, to find instructions on cleaning up resources, see [Clean Up](https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-cleanup.html) in the *Amazon SageMaker Developer Guide*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfdef3c",
   "metadata": {},
   "source": [
    "## Conslusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8db2c73",
   "metadata": {},
   "source": [
    "We conduct a systematic, large scale ViT transfer learning for clinic image claasification using native, SageMaker Traioning Compilor and SageMaker Distributed Training when pre-training Vision Transformers, including their respective effects on the compute budget needed to achieve a certain level of performance. We also evaluate pre-trained models through the lens of transfer learning. As a result, we characterize a quite complex landscape of training settings for pre-training Vision Transformers across different model sizes. Our experiments yield a number of surprising insights around the impact of various techniques and the situations when augmentation and regularization are beneficial and when not.\n",
    "\n",
    "We also perform an in-depth analysis of the transfer learning setting for Vision Transformers. We conclude that across a wide range of datasets, even if the downstream data of interest appears to only be weakly related to the data used for pre-training, transfer learning remains the best available option. Our analysis also suggests that among similarly performing pre-trained models, for transfer learning a model with more training data should likely be preferred over one with more data augmentation.\n",
    "\n",
    "We hope that our study will help guide future research on Vision Transformers and will be a useful source of effective training settings for practitioners seeking to optimize their final model performance in the light of a given computational budget."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
