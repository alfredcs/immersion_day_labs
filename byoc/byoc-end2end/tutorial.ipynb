{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End to End Pipeline: Bring your own container to SageMaker pipelines\n",
    "This notebook walks you through Bring your own container to [Amazon SagMaker Pipelines](https://aws.amazon.com/sagemaker/pipelines/) with [Hugging Face](https://huggingface.co/). Hugging Face containes plenty of NLP taskes such as text classitifation, summarization, text generation. In this tutorial, we take text classificaiton as an example.\n",
    "\n",
    "### Overview\n",
    "<div align=\"center\"><img src=\"images/byoc_mlops_nb.png\" /></div>\n",
    "\n",
    "---\n",
    "- [1.Prepare the environment](#envpreparation)\n",
    "- [2.Data preparation](#datapreparation)\n",
    "- [3.Feature ingestion](#featureingestion)\n",
    "- [4.Model building](#modelbuilding)\n",
    "- [5.Asynchronous inference](#asyncInference)\n",
    "- [6.Real-time inference](#realtimeInference)\n",
    "- [7.Cleanup](#cleanup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"envpreparation\"></a>\n",
    "## 1. Prepare the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! pip install --upgrade pip\n",
    "! python3 -m pip install sagemaker==2.72.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import io\n",
    "import glob\n",
    "import os\n",
    "import string\n",
    "import re\n",
    "import time\n",
    "from time import strftime,gmtime\n",
    "from botocore.exceptions import ClientError\n",
    "import urllib\n",
    "import sys\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(name='byoc-pipeline')\n",
    "sagemaker_session = sagemaker.Session()\n",
    "boto_session = sagemaker_session.boto_session\n",
    "sagemaker_client = boto_session.client('sagemaker')\n",
    "sm_runtime = boto3.Session().client('sagemaker-runtime')\n",
    "region = sagemaker_session.boto_region_name\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "client = boto3.client('sts')\n",
    "account = client.get_caller_identity()['Account']\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = \"sm-pipeline-huggingface\"\n",
    "task_name = \"text-classification\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Docker Environment Preparation\n",
    "Because the volume size of container is larger than available size in root of Notebook Jupyter instance, we need to put the directory of docker data into ```/home/ec2-user/SageMaker/docker```.\n",
    "\n",
    "By default, data root of docker is set as ```/var/lib/docker/```. we need to change the directory of docker to ```/home/ec2-user/SageMaker/docker```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat /etc/docker/daemon.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash ./scripts/prepare-docker.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"datapreparation\"></a>\n",
    "## 2. Data prepration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the data from [Standord AI Lab](https://ai.stanford.edu/~amaas/data/sentiment/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "!wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz -O aclImdb_v1.tar.gz\n",
    "!tar --no-same-owner -xvzf aclImdb_v1.tar.gz\n",
    "!rm aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stage data for ingesting dataset into feature ingestion. [SageMaker Feature Store](https://aws.amazon.com/sagemaker/feature-store/) only accept text without punctuation, we need to remove punctuation on the raw data. This process is necessary when you use your own data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "punc_list = string.punctuation  # you can self define list of punctuation to remove here\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    \"\"\"\n",
    "    This function takes strings containing self defined punctuations and returns\n",
    "    strings with punctuations removed.\n",
    "    Input(string): one tweet, contains punctuations in the self-defined list\n",
    "    Output(string): one tweet, self-defined punctuations removed\n",
    "    \"\"\"\n",
    "    translator = str.maketrans(\"\", \"\", punc_list)\n",
    "    return text.translate(translator)\n",
    "\n",
    "def staging_data(data_dir):\n",
    "    for data_type in [\"train\", \"test\"]:\n",
    "        data_list = []\n",
    "        for label in [\"neg\", \"pos\"]:\n",
    "            data_path = os.path.join(data_dir, data_type, label)\n",
    "            for files in glob.glob(data_path + '/*.txt'):\n",
    "                data_id = files.split('/')[-1].replace('.txt', '')\n",
    "                with open(files, 'r') as f:\n",
    "                    line = f.readline()\n",
    "                    line = remove_punctuation(line)\n",
    "                    line = re.sub(\"\\s+\", \" \", line)\n",
    "                    data_list.append([data_id, line, label])\n",
    "                    \n",
    "        data_df = pd.DataFrame(data_list, columns=[\"index\", \"text\", \"label\"])\n",
    "        data_df[\"event_time\"] = time.time()\n",
    "        data_df[\"data_type\"] = data_type\n",
    "        #data_df.reset_index(inplace=True)\n",
    "        data_df.to_csv(f'{data_dir}/{data_type}.csv', index=False)\n",
    "\n",
    "data_dir = f\"{os.getcwd()}/aclImdb\"\n",
    "staging_data(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(os.path.join(data_dir, 'train.csv'))\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload data to S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp ./aclImdb/train.csv s3://$bucket/$prefix/raw_data/imdb_train.csv\n",
    "!aws s3 cp ./aclImdb/test.csv s3://$bucket/$prefix/raw_data/imdb_test.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create test data for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data_dir = \"./data\"\n",
    "if not os.path.exists(sample_data_dir):\n",
    "    os.makedirs(sample_data_dir)\n",
    "train_df[\"text\"][:10].to_csv(f\"{sample_data_dir}/sample_imdb.csv\", header=None, index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"featureingestion\"></a>\n",
    "## 3. Feature ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We ingest dataset into feature store with `SageMaker processing`. Alternatively, you can complete this task with `SageMaker Data Wrangler`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Create feature group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.astype({\n",
    "    \"index\": \"string\",\n",
    "    \"text\": \"string\",\n",
    "    \"label\": \"string\",\n",
    "    \"data_type\": \"string\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create feature group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.feature_store.feature_group import FeatureGroup\n",
    "\n",
    "feature_group_name = f\"hugging-face-imdb-{strftime('%Y-%m-%d-%H-%M-%S', gmtime())}\"\n",
    "imdb_feature_group = FeatureGroup(name=feature_group_name, sagemaker_session=sagemaker_session)\n",
    "imdb_feature_group.load_feature_definitions(data_frame=train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_for_feature_group_creation_complete(feature_group):\n",
    "    status = feature_group.describe().get('FeatureGroupStatus')\n",
    "    print(f'Initial status: {status}')\n",
    "    while status == 'Creating':\n",
    "        logger.info(f'Waiting for feature group: {feature_group.name} to be created ...')\n",
    "        time.sleep(5)\n",
    "        status = feature_group.describe().get('FeatureGroupStatus')\n",
    "    if status != 'Created':\n",
    "        raise SystemExit(f'Failed to create feature group {feature_group.name}: {status}')\n",
    "    logger.info(f'FeatureGroup {feature_group.name} was successfully created.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_feature_group.create(s3_uri=f's3://{bucket}/{prefix}/feature_store', \n",
    "                               record_identifier_name='index', \n",
    "                               event_time_feature_name='event_time', \n",
    "                               role_arn=role, \n",
    "                               enable_online_store=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wait_for_feature_group_creation_complete(imdb_feature_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove IMDB dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r aclImdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Feature engineering with SageMaker preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.spark.processing import PySparkProcessor\n",
    "\n",
    "s3_uri_prefix = f's3://{bucket}/{prefix}/raw_data/*'\n",
    "\n",
    "pyspark_processor = PySparkProcessor(framework_version='2.4', # Spark version\n",
    "                                     role=role,\n",
    "                                     instance_type='ml.m5.xlarge',\n",
    "                                     instance_count=2,\n",
    "                                     base_job_name='sm-processing-pyspark-fs-ingestion',\n",
    "                                     env={'AWS_DEFAULT_REGION': boto3.Session().region_name,\n",
    "                                          'mode': 'python'},\n",
    "                                     max_runtime_in_seconds=3600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pyspark_processor.run(submit_app='./processing/batch_ingest_sm_pyspark.py', \n",
    "                      arguments = ['--feature_group_name', feature_group_name, \n",
    "                                   '--s3_uri_prefix', s3_uri_prefix], \n",
    "                      spark_event_logs_s3_uri=f's3://{bucket}/{prefix}/spark-logs', \n",
    "                      logs=False)  # set logs=True to enable logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Verify feature ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestore_runtime = boto_session.client(\n",
    "    service_name=\"sagemaker-featurestore-runtime\", region_name=region\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = featurestore_runtime.get_record(\n",
    "        FeatureGroupName=feature_group_name,\n",
    "        RecordIdentifierValueAsString=\"3174_4\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record = response[\"Record\"]\n",
    "df = pd.DataFrame(record).set_index('FeatureName').transpose()\n",
    "df[\"text\"].tolist()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"modelbuilding\"></a>\n",
    "## 4. Model Building\n",
    "With data in the feature store, you can now start the model building pipeline. You can leave the default parameter values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Build and push your own docker to ECR\n",
    "\n",
    "We preprare the three containers respectively for training, batch inference and real-time inference.\n",
    "\n",
    "Build and push training image to ECR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_name = f\"sagemaker-{task_name}-training\"\n",
    "tag = \"tf2.5.1\"\n",
    "training_image = f\"{account}.dkr.ecr.{region}.amazonaws.com/{train_image_name}:{tag}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!bash containers/training/build_tools/build_and_push.sh {region} {train_image_name} {tag}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Build and push serving image to ECR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serving_image_name = f\"sagemaker-{task_name}-serving\"\n",
    "tag = \"tf2.5.1\"\n",
    "serving_image = f\"{account}.dkr.ecr.{region}.amazonaws.com/{serving_image_name}:{tag}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!bash containers/serving/build_tools/build_and_push.sh {region} {serving_image_name} {tag}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Build and push batch inference image to ECR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_inference_image_name = f\"sagemaker-{task_name}-batch-inference\"\n",
    "tag = \"tf2.5.1\"\n",
    "batch_inference_image = f\"{account}.dkr.ecr.{region}.amazonaws.com/{batch_inference_image_name}:{tag}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!bash containers/batch_transform/build_tools/build_and_push.sh {region} {batch_inference_image_name} {tag}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Define SageMaker pipeline for model building and model registy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipelines.byoc_pipeline import create_pipeline\n",
    "import pipelines.byoc_pipeline\n",
    "import importlib\n",
    "importlib.reload(pipelines.byoc_pipeline)\n",
    "\n",
    "model_package_group_name = \"huggingfaceImdb\"\n",
    "database_name = \"huggingface_imdb_featurestore\"\n",
    "\n",
    "pipeline_configuration = {\n",
    "        \"fg_name\": feature_group_name,\n",
    "        \"create_dataset_script_path\": \"processing/create_dataset.py\",\n",
    "        \"prefix\": f\"{prefix}_byoc_build\",\n",
    "        \"database_name\": database_name,\n",
    "        \"model_package_group_name\": model_package_group_name,\n",
    "        \"model_accuracy_threshold\": 0.9,\n",
    "        \"containers\": {\n",
    "            \"training_docker_image\": training_image,\n",
    "            \"endpoint_docker_image\": serving_image,\n",
    "            \"transform_docker_image\": batch_inference_image\n",
    "        },\n",
    "        \"metric_definitions\": [\n",
    "            {\n",
    "                \"Name\": \"loss\",\n",
    "                \"Regex\": \"loss': ([0-9\\\\.]+)\"\n",
    "            },\n",
    "            {\n",
    "                \"Name\": \"learning_rate\",\n",
    "                \"Regex\": \"learning_rate': ([0-9e\\\\-\\\\.]+)\"\n",
    "            },\n",
    "            {\n",
    "                \"Name\": \"eval_loss\",\n",
    "                \"Regex\": \"eval_loss': ([0-9e\\\\-\\\\.]+)\"\n",
    "            },\n",
    "            {\n",
    "                \"Name\": \"eval_accuracy\",\n",
    "                \"Regex\": \"eval_accuracy': ([0-9e\\\\-\\\\.]+)\"\n",
    "            }\n",
    "        ],\n",
    "        \"hpo_configuration\":{\n",
    "            \"objective_metric\": \"eval_accuracy\",\n",
    "            \"max_jobs\": 1,\n",
    "            \"max_parallel_jobs\": 1,\n",
    "            \"strategy\": \"Bayesian\",\n",
    "            \"objective_type\": \"Maximize\",\n",
    "            \"param_ranges\": {\n",
    "                \"ContinuousParameter\": [\n",
    "                    {\n",
    "                        \"Name\": \"learning_rate\",\n",
    "                        \"MaxValue\": 5e-3,\n",
    "                        \"MinValue\": 5e-6,\n",
    "                        \"ScalingType\": \"Logarithmic\"\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "            \"static_hyperparameters\": {\n",
    "                \"weight_decay\": 0.01,\n",
    "                \"per_device_train_batch_size\": 16,\n",
    "                \"per_device_eval_batch_size\": 32,\n",
    "                \"num_train_epochs\": 10,\n",
    "                \"warmup_steps\": 500,\n",
    "                \"logging_steps\": 10,\n",
    "                \"eval_steps\": 500,\n",
    "                \"tokenizer_download_model\": \"enable\"\n",
    "            }\n",
    "        },\n",
    "        \"hyperparameters\": {\n",
    "            \"weight_decay\": 0.01,\n",
    "            \"per_device_train_batch_size\": 16,\n",
    "            \"per_device_eval_batch_size\": 32,\n",
    "            \"num_train_epochs\": 10,\n",
    "            \"warmup_steps\": 500,\n",
    "            \"logging_steps\": 10,\n",
    "            \"eval_steps\": 500,\n",
    "            \"learning_rate\": 5e-5\n",
    "        },\n",
    "        \"feature_names\": [\"index\", \"text\", \"data_type\"],\n",
    "        \"label_name\": [\"label\"]\n",
    "    }\n",
    "\n",
    "pipeline = create_pipeline(role, \"huggingface-pipeline\", sagemaker_session, **pipeline_configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.upsert(role_arn=role)\n",
    "\n",
    "parameters = {\n",
    "    \"TrainingInstance\": \"ml.p3.16xlarge\",\n",
    "    \"ProcessingInstanceType\": \"ml.m5.xlarge\"\n",
    "}\n",
    "\n",
    "start_response = pipeline.start(parameters=parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the execution status in SageMaker studio. It takes about 1 hour to complete pipeline execution with the default parameters. If you would like to run more training jobs or run more epochs, the parameters `delay` and `max_attemps` need to be modified to a larger value to make sure `delay * max_attemps` is larger than the total training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_response.wait(delay=30, max_attempts=180)\n",
    "start_response.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract arn of model package from meta data store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = sagemaker_client.list_model_packages(ModelPackageGroupName=model_package_group_name)[\"ModelPackageSummaryList\"]\n",
    "model_package_arn = model_list[0][\"ModelPackageArn\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the objective metric `accuracy` is larger than defined threshold, model will be registried into model registry. By setting the input parameter `ModelApprovalStatus` to `Approved`, the latest model can be accessible to inference endpoint. Manually setting the model status to `Approved` is required if you set `ModelApprovalStatus` to `PendingManualApproval` as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_package_update_input_dict = {\n",
    "    \"ModelPackageArn\" : model_package_arn,\n",
    "    \"ModelApprovalStatus\" : \"Approved\"\n",
    "}\n",
    "model_package_update_response = sagemaker_client.update_model_package(**model_package_update_input_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a model for serving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = f\"huggingface-{strftime('%Y-%m-%d-%H-%M-%S', gmtime())}\"\n",
    "print(\"Model name : {}\".format(model_name))\n",
    "container_list = [{'ModelPackageName': model_package_arn}]\n",
    "\n",
    "create_model_response = sagemaker_client.create_model(\n",
    "    ModelName = model_name,\n",
    "    ExecutionRoleArn = role,\n",
    "    Containers = container_list\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"asyncInference\"></a>\n",
    "## 5. Asynchronous inference\n",
    "\n",
    "[Amazon SageMaker Asynchronous Inference](https://docs.aws.amazon.com/sagemaker/latest/dg/async-inference.html) is a new capability in SageMaker that queues incoming requests and processes them asynchronously. This option is ideal for requests with large payload sizes up to 1GB, long processing times, and near real-time latency requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Deploy asynchronous endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async_endpoint_config_name = f\"BYOCAsyncEndpointConfig-{strftime('%Y-%m-%d-%H-%M-%S', gmtime())}\"\n",
    "\n",
    "create_endpoint_config_response = sagemaker_client.create_endpoint_config(\n",
    "    EndpointConfigName=async_endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": \"variant1\",\n",
    "            \"ModelName\": model_name,\n",
    "            \"InstanceType\": \"ml.m5.large\",\n",
    "            \"InitialInstanceCount\": 1\n",
    "        }\n",
    "    ],\n",
    "    AsyncInferenceConfig={\n",
    "        \"OutputConfig\": {\n",
    "            \"S3OutputPath\": f\"s3://{bucket}/{prefix}/output\",\n",
    "            #  Optionally specify Amazon SNS topics\n",
    "            #\"NotificationConfig\": {\n",
    "            #  \"SuccessTopic\": success_topic,\n",
    "            #  \"ErrorTopic\": error_topic,\n",
    "            #}\n",
    "        },\n",
    "        \"ClientConfig\": {\n",
    "            \"MaxConcurrentInvocationsPerInstance\": 2\n",
    "        }\n",
    "    }\n",
    ")\n",
    "print(f\"Created EndpointConfig: {create_endpoint_config_response['EndpointConfigArn']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async_endpoint_name = f\"byoc-{strftime('%Y-%m-%d-%H-%M-%S', gmtime())}\"\n",
    "create_endpoint_response = sagemaker_client.create_endpoint(EndpointName=async_endpoint_name, EndpointConfigName=async_endpoint_config_name)\n",
    "print(f\"Creating Endpoint: {create_endpoint_response['EndpointArn']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waiter = boto3.client('sagemaker').get_waiter('endpoint_in_service')\n",
    "print(\"Waiting for endpoint to create...\")\n",
    "waiter.wait(EndpointName=async_endpoint_name)\n",
    "resp = sagemaker_client.describe_endpoint(EndpointName=async_endpoint_name)\n",
    "print(f\"Endpoint Status: {resp['EndpointStatus']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enable autoscaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = boto3.client('application-autoscaling') # Common class representing Application Auto Scaling for SageMaker amongst other services\n",
    "\n",
    "resource_id='endpoint/' + async_endpoint_name + '/variant/' + 'variant1' # This is the format in which application autoscaling references the endpoint\n",
    "\n",
    "response = client.register_scalable_target(\n",
    "    ServiceNamespace='sagemaker', \n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension='sagemaker:variant:DesiredInstanceCount',\n",
    "    MinCapacity=0,  \n",
    "    MaxCapacity=5\n",
    ")\n",
    "\n",
    "response = client.put_scaling_policy(\n",
    "    PolicyName='Invocations-ScalingPolicy',\n",
    "    ServiceNamespace='sagemaker', # The namespace of the AWS service that provides the resource. \n",
    "    ResourceId=resource_id, # Endpoint name \n",
    "    ScalableDimension='sagemaker:variant:DesiredInstanceCount', # SageMaker supports only Instance Count\n",
    "    PolicyType='TargetTrackingScaling', # 'StepScaling'|'TargetTrackingScaling'\n",
    "    TargetTrackingScalingPolicyConfiguration={\n",
    "        'TargetValue': 5.0, # The target value for the metric. \n",
    "        'CustomizedMetricSpecification': {\n",
    "            'MetricName': 'ApproximateBacklogSizePerInstance',\n",
    "            'Namespace': 'AWS/SageMaker',\n",
    "            'Dimensions': [\n",
    "                {'Name': 'EndpointName', 'Value': async_endpoint_name }\n",
    "            ],\n",
    "            'Statistic': 'Average',\n",
    "        },\n",
    "        'ScaleInCooldown': 120, # The cooldown period helps you prevent your Auto Scaling group from launching or terminating \n",
    "                                # additional instances before the effects of previous activities are visible. \n",
    "                                # You can configure the length of time based on your instance startup time or other application needs.\n",
    "                                # ScaleInCooldown - The amount of time, in seconds, after a scale in activity completes before another scale in activity can start. \n",
    "        'ScaleOutCooldown': 120 # ScaleOutCooldown - The amount of time, in seconds, after a scale out activity completes before another scale out activity can start.\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Tesing batch inference with asynchronous inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload sample data to s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_s3_location = f\"s3://{bucket}/{prefix}/sample_data/sample_imdb.csv\"\n",
    "\n",
    "!aws s3 cp ./data/sample_imdb.csv $input_s3_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = sm_runtime.invoke_endpoint_async(\n",
    "    EndpointName=async_endpoint_name, \n",
    "    InputLocation=input_s3_location\n",
    ")\n",
    "output_location = response['OutputLocation']\n",
    "print(f\"OutputLocation: {output_location}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output(output_location):\n",
    "    output_url = urllib.parse.urlparse(output_location)\n",
    "    bucket = output_url.netloc\n",
    "    key = output_url.path[1:]\n",
    "    while True:\n",
    "        try:\n",
    "            return sagemaker_session.read_s3_file(bucket=output_url.netloc, key_prefix=output_url.path[1:])\n",
    "        except ClientError as e:\n",
    "            if e.response['Error']['Code'] == 'NoSuchKey':\n",
    "                print(\"waiting for output...\")\n",
    "                time.sleep(2)\n",
    "                continue\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = get_output(output_location)\n",
    "print(f\"Output size in bytes: {((sys.getsizeof(output)))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we confirm the result of asynchronous inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async_infer_res = \"./data/async_res.json\"\n",
    "\n",
    "!aws s3 cp $output_location $async_infer_res\n",
    "\n",
    "with open(async_infer_res, 'r') as f:\n",
    "    async_res = json.load(f)\n",
    "async_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"realtimeInference\"></a>\n",
    "## 6. Real-time inference\n",
    "[Real-time inference](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints.html) is ideal for inference workloads where you have real-time, interactive, low latency requirements. \n",
    "\n",
    "### 6.1 Deploy real-time inference endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_config_name = f\"BYOCEndpointConfig-{strftime('%Y-%m-%d-%H-%M-%S', gmtime())}\"\n",
    "\n",
    "create_endpoint_config_response = sagemaker_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": \"variant1\",\n",
    "            \"ModelName\": model_name,\n",
    "            \"InstanceType\": \"ml.m5.large\",\n",
    "            \"InitialInstanceCount\": 1\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "print(f\"Created EndpointConfig: {create_endpoint_config_response['EndpointConfigArn']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = f\"byoc-{strftime('%Y-%m-%d-%H-%M-%S', gmtime())}\"\n",
    "create_endpoint_response = sagemaker_client.create_endpoint(EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name)\n",
    "print(f\"Creating Endpoint: {create_endpoint_response['EndpointArn']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waiter = boto3.client('sagemaker').get_waiter('endpoint_in_service')\n",
    "print(\"Waiting for endpoint to create...\")\n",
    "waiter.wait(EndpointName=endpoint_name)\n",
    "resp = sagemaker_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "print(f\"Endpoint Status: {resp['EndpointStatus']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Testing real-time inference endpoint\n",
    "Load sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = pd.read_csv('./data/sample_imdb.csv', header=None)\n",
    "sample_df.columns = [\"text\"]\n",
    "sample_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Error will occur when parameter of request is too long, where asynchronous inference would be an alternative. We pick out the first 5 rows to do real-time inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_list = sample_df[\"text\"].values.tolist()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_record = pd.DataFrame({\"inputs\": sample_list})\n",
    "csv_file = io.StringIO()\n",
    "df_record.to_csv(csv_file, sep=\",\", header=False, index=False)\n",
    "payload_as_csv = csv_file.getvalue()\n",
    "\n",
    "response = sm_runtime.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    Body= payload_as_csv,\n",
    "    ContentType = 'text/csv'\n",
    ")\n",
    "\n",
    "body = response[\"Body\"].read()\n",
    "msg = body.decode(\"utf-8\")\n",
    "data = json.loads(msg)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the predicted results from real-time inference is identical as those from asynchronous inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"cleanup\"></a>\n",
    "## 7. Cleanup\n",
    "\n",
    "Delete resources(endpoint, model, s3, docker image) we created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scripts.utils import delete_endpoint, delete_model, delete_s3, delete_ecr, delete_fg\n",
    "\n",
    "# delete endpoints\n",
    "delete_endpoint(sagemaker_client, async_endpoint_name, async_endpoint_config_name)\n",
    "delete_endpoint(sagemaker_client, endpoint_name, endpoint_config_name)\n",
    "\n",
    "# delete models\n",
    "delete_model(sagemaker_client, model_name, model_package_arn, model_package_group_name)\n",
    "\n",
    "# delete feature group\n",
    "delete_fg(sagemaker_client, feature_group_name)\n",
    "\n",
    "# delete data in s3\n",
    "delete_s3(bucket, prefix)\n",
    "\n",
    "# delete docker image in ECR\n",
    "for image_name in [train_image_name, serving_image_name, batch_inference_image_name]:\n",
    "    delete_ecr(image_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete local docker images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!docker rmi -f $(docker images -a | grep text-classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
